# IT伦理与道德研究 - AI 伦理准则
## 1.欧盟呼吁共建 AI 伦理准则
随着智能时代的到来，机器人越来越多的参与到我们的生活和工作中，而人工智能（Artificial Intelligence，缩写AI）就是对人意识、思维的信息过程的模拟，人工智能机器人就是类似人类能自我意识的机器。  
这是一个极富挑战的科学领域，而挑战意味着我们面临着不少待解决的问题，AI科技评论（aitechtalk）在[《为什么欧盟呼吁共建 AI 伦理准则？》](https://cloud.tencent.com/developer/article/1079921)中提到
>下面这些问题，可能在我们不久的将来（一年或者两年？）就会出现在我们身边：  
1.目前人工智能正逐渐大规模地应用于**医疗行业**，许多疾病的诊断已经可以通过机器学习的方法来完成。但是我们设想，如果人工智能诊断出现医疗误判，最终导致医疗事故，那么谁将来承担事故责任呢？是提供技术的公司，还是医院，或者医生个人？  
2.**自动驾驶**正逐渐上路，据各方估计在2020年之前自动驾驶汽车将进入消费市场。不可避免的，自动驾驶汽车在不熟悉的环境中可能会导致交通事故（例如2016年的特斯拉自动驾驶汽车事故），那么在这种情况下谁将承担法律责任呢？是车主，还是车辆生产方？  
3.随着人工智能系统以及机器人研究的发展，**致命自主武装系统**逐渐应用于军事当中。当这些自主武装系统在前线失控误杀平民（甚至军人）时，谁将承担这些责任呢？  
4.随着人工智能的发展，**技术的垄断**越来越严重，政府和大企业在逐渐垄断着技术和数据，他们一方面利用这些数据给我们的生活带来便利，另一方面也在某种程度上控制着我们的生活。那么技术是否应该开源开放呢？在什么样的标准上我们的人权才能得以保障呢？  
…………

随着人工智能在人们生活中应用越来越广泛，也有越来越多的人对这些问题开始思考和制定规则。在过去的两年时间里，有大量的 AI 研究人员开启对 AI 伦理的思索。

>2016 年 9 月，Amazon、Facebook、Google、DeepMind、Microsoft 和 IBM 等几家全球大公司共同成立了 Partnership on AI，来研究和制定人工智能技术的最佳实践，促进公众对人工智能的理解，并作为一个开放平台来讨论人工智能对人和社会的影响。
![](https://ask.qcloudimg.com/http-save/yehe-1326493/otxdksa5oc.jpeg?imageView2/2/w/1620)

>2016 年 12 月，IEEE 首家发布了第一版《合乎伦理的设计：将人类福祉与人工智能和自主系统优先考虑的愿景》，由全球 100 多名人工智能、伦理学及相关领域的思想领袖和专家共同制定。
![](https://ask.qcloudimg.com/http-save/yehe-1326493/kxefc58hec.jpeg?imageView2/2/w/1620)
https://autonomousweapons.org/

>2017 年 6 月，ITU（国际电信联盟）举办了「AI for Good」的全球峰会。今年五月将在日内瓦再次举办同一主题的峰会。
![](https://ask.qcloudimg.com/http-save/yehe-1326493/ix63vtwwqr.jpeg?imageView2/2/w/1620)

>2017 年 11 月，蒙特利尔大学举办「人工智能的社会责任发展」论坛，随后并发表了《人工智能负责任开发宣言》  
>
>2017 年，人类未来研究所发布的《阿西洛马人工智能原则》（共23条）。有 1273 名人工智能 /机器人研究人员和其他 2541 人签署并支持了这些原则。  
>
>此外，2016 年联合国在「特定常规武器公约」会议上启动了关于人工智能在军事使用上的全球性辩论，其中大多数缔约方赞同了所谓的「对致命自主武器系统进行有意义的人类控制原则」， 这条原则提出「凡是无有意义的人类控制的致命自主武器系统都应被禁止」。联合国还在海牙建立了一个专门的研究机构（犯罪和司法研究所），主要用来研究机器人和人工智能治理的问题。


但是上述的这些伦理思考，一方面并没有经过严格地思考和准确地定义；另一方面，在国家层次上关于 AI 的规则制定并不平衡，有的国家已经优先制定了机器人和人工智能的规则，甚至还通过立法来管理自动驾驶汽车；但大多数国家还没有着手这样的事情。

为了解决这样的问题，近期欧盟的「欧洲科学和新技术伦理小组」（European Group on Ethics in Science and New Technologies，EGE）发布了一份声明，希望各国能够合作起来共同制定标准和规则，并呼吁对 AI、机器人以及自主技术的伦理学进行广泛和系统地讨论。

![](https://ask.qcloudimg.com/http-save/yehe-1326493/ml549qssxv.jpeg?imageView2/2/w/1620)

在 24 页的声明中，EGE 介绍了 AI 伦理的背景、面临的问题以及重要的思考因素。它认为我们应当跳出狭义的伦理框架思考。例如，到底是什么使我们陷入前文中提到的伦理困境？什么才算「人类控制」？人们是否应当有权知道某个系统是自主系统还是人工系统？……

为了制定一套可作为建立全球标准和立法行动基础的道德准则，作为第一步，EGE 提出了一套基于《欧盟条约》和《欧盟基本权利宪章》的基本原则和民主先决条件。

>基本原则和民主先决条件  
>a）人类尊严  
人类尊严的原则，即尊重个人的生命和人格，不被人工智能技术所侵犯。举例来说，当人与一个系统进行交互时，我们应该明确被告知与我们交流的是机器，而不是真人。也即我们应当有知情权，知道我们是否以及什么时候在与机器（或另一个人类）交流。
>
>b）自主  
>自主原则意味着人的自由。对人工智能系统的控制和了解应当成为人的责任，智能系统不得损害人类自由，不得设定自己的标准和规范。所有的智能技术必须尊重人类选择是否、何时以及如何将决策和行动委托给它们。
>
>c）责任  
责任原则必须成为人工智能研究和应用的基础。智能系统的开发和应用必须为全球社会和环境的利益服务，因此它们的设计应该与人类的价值观和权利相一致。由于智能系统的潜在滥用可能会对人类整体构成重大的威胁，因此风险意识和预防措施至关重要。
>
>d）正义、平等和团结    
应当利用人工智能的优势为全球正义、平等做贡献。应尽早预防、检测、报告和消除用于训练人工智能系统的数据集中的歧视性偏差。我们需要协调一致的全球努力，实现平等获得智能系统技术，在社会内部公平分配利益和机会。
>
>e）民主  
关于人工智能发展和应用管理的关键决定应通过民主辩论和公众参与来决定。在这些问题上进行全球合作和公共对话，将确保这些问题以包容、知情和远见的方式被采纳。接受教育或获取有关技术信息的权利将有助于每个人都了解风险和机遇，并有权参与决策过程。
>
>f）法治和问责制  
健全人工智能时代的法律，确保降低可能侵犯人权的智能系统产生的风险（如安全和隐私）。应尽快投入开发有效的解决方案，以提供公平、明确的责任分配和有约束力的法律机制，解决该领域出现的所有法律挑战。
在这方面，各国政府和国际组织应加大力度，明确智能系统的不良行为所造成的损失该由谁来承担责任。
>
>g）安全保障、身心完整  
智能系统的安全保障体现为三种形式：  
（1）其环境和用户的外部安全；  
（2）可靠性以及内部稳健性，例如抗黑客攻击能力；  
（3）人机交互方面的情感安全。AI开发人员必须考虑到安全性的所有方面，并在发布之前进行严格测试，以确保智能系统不会侵犯身心完整（bodily and mental integrity）和安全保障（safety and security）的人权，尤其需要注意处于弱势地位的人。
>
>h）数据保护和隐私  
在数字通信技术无处不在，且能够大量收集数据的时代，保护个人信息和尊重隐私权的权力受到了严重的挑战。作为物联网一部分的AI机器人，以及通过互联网运行的 AI 软件必须遵守数据保护条例，而不是收集和传播数据，或运行没有给予同意的数据集。  
智能系统不得干涉私人生活权，其中包括免受技术影响个人发展和发表意见的权利，建立和发展与他人关系的权利，以及免于监视的权利。  
鉴于智能系统对私人生活和隐私的影响，可考虑引入两项新的权利：有意义的人与人之间的接触权，以及不被推荐、测量、分析、指导和干扰的权利。
>
>i）可持续发展  
人工智能技术必须符合人类的责任，以确保我们星球上生命的基本先决条件，持续促进人类的繁衍以及为后代保持良好的环境。为了防止未来技术对人类生命和自然造成不利影响，制定的策略应基于环境保护和可持续发展为优先的策略。

## 2. 人工智能伦理问题 - 道德问题
腾讯研究院法律研究中心高级研究员曹建峰，腾讯研究院法律研究中心助理研究员付一方、霍文新在[《人工智能伦理问题》](https://mp.weixin.qq.com/s?__biz=MjM5OTE0ODA2MQ==&mid=2650879294&idx=1&sn=282e2d18f1c5abf2d088312913ef5ee8&chksm=bcca7a4c8bbdf35a3a2754dcd4b5bdf0a24658d47a4d45901d3d26716bca34d1f67985389815#rd)
一文中提到在讨论人工智能时我们无法避开的道德问题：

>现阶段值得我们进行反思的是，随着能够脱离人类而独立运行的高科技系统和软件的出现，一些人类工作即将被取代，这将引发一系列重要且艰难的道德问题。
>
>1.关键问题
>
>第一，是关于安全性、保险性以及预防损害、减少风险的问题。我们如何才能让互联的人工智能和“自主”设备安全可靠地运行，我们又该如何评估风险？  
>第二，是关于人类道德责任方面的问题。在具有先进人工智能和机器人组件的动态和复杂的社会-技术系统中，道德相关的代理（agency）应该处于什么位置？应该如何分担产生的道德责任，又应该由谁为其中产生的不良后果负责？探讨人与智能机器之间的“共同控制”和“共同责任”是否有意义？人类是否将成为被视为道德“缓冲区（crumple zones）”的“自主”设备的生态系统的一部分，是为了吸收责任，还是他们有能力为自己的行为负责?  
>第三，人工智能引发了关于治理、监管、设计、开发、检查、监督、测试和认证的问题。我们应该如何重新设计为个人和社会服务的机构和法律，以确保这一技术不会给社会带来危害？  
>第四，是有关民主决策的问题，主要是关于制度、政策以及价值观的决策，这些是解决上述所有问题的基础。在全球范围内开展调查，确定公民利用基于机器学习、大数据和行为科学相结合的先进技术的程度，确定其是否能够利用这些先进技术进行细致的分析和定位，或者根据商业或政治目的进行合适的架构选择。  
>最后，对人工智能和“自主”系统的可解释性和透明度仍存在疑问。这些系统能够有效地提供哪些价值？我们应基于何种价值观来设计机器，制定政策？我们在技术进步和效益权衡方面是否直接地或间接地破坏了一部分价值？  
>一些国家关于人工智能驱动的，基于社会评分系统的，社会过程的“最优化”的试验，违背了平等和自由的基本思想，这就像种姓制度一样，在现实中定义了“不同种类的人”以及“人的不同属性”。
>
>2.核心考量
>
>道德意义上的自主只能是一种人类活动。所以，将“自主”这个术语应用于纯粹的人工制品，是一种误用。然而，“自主”系统这个术语在科学文献和公众辩论中却广泛流行，指的是在操作和决策“自主”方面最高程度的自动性和独立性。但是，原始意义上的自主是人类尊严的一个重要方面，不应该将其相对化。  
>机器不能被赋予人的道德地位并且继承人的尊严。因此，与生产的自动化相反，用我们处理物体或数据的方式来解决人类问题是不合适的，这种对人类“自主”的处理方式是不道德的，并且会破坏本质意义上的欧洲价值观。  
>人们应该明确哪些价值是可以由技术服务的，哪些是与道德紧密相关的，不管机器会变得多么“自主”，有些问题是不能由它们来解决的。  
>这里的道德责任可能被广义地解释为涉及人类代理的几个方面，例如因果关系、问责制（提供说明的义务）、责任（赔偿损害的义务）、赞赏和责备等反应性态度以及与社会角色相关的责任。无论如何，道德责任不能被分配或转移到相关的“自主”技术上。  
>最近，在关于致命自主武器系统（Lethal Autonomous Weapons Systems）和自主车辆的争论中，似乎存在着广泛的共识，即有意义的人类控制对道德责任至关重要。
这是有意义的人类控制（Meaningful Human Control）原则首次被提出用来限制未来武器系统的开发和利用。这意味着人类——而不是计算机及其算法——最终应在此类问题上保持控制权。
>
>3. 超越狭义道德框架（ethical framing）
>
>自动驾驶汽车和致命自主武器系统的发展引发了众多关于道德的辩论。该框架的核心问题主要是“自主”系统的责任问题，其可能造成的影响以及如何对其进行编程，使其能够在应用中符合道德规则，如在危急情况下挽救人的生命。  
这其实忽略了很多问题，例如“过去采取的哪些设计方面的决策造成了这种道德困境”，“哪些价值观应该在设计中予以考虑”，“设计中的价值应该在冲突情况下如何衡量以及由谁来衡量”？   
第二个存在争议的是“自主”武器系统。现在需要关注的问题是这些系统上“有意义的人类控制”的性质和意义是什么，以及如何建立这种符合道德原则的“有意义的人类控制”。  
第三个重要的应用领域是“自主”软件，包括机器人。除了关于数据保护和隐私的简单问题之外，我们可能会问人们是否有权知道他们是在和人还是和人工智能系统交流。  
>
>此外，问题还在于是否应该限制人工智能系统根据个人对其身份概念的建构而向该人提供建议的行为。虽然人们越来越意识到需要解决这些问题，但目前人工智能和机器人技术的发展速度远快于寻找这些棘手的道德、法律和社会问题答案的速度。显然现在急需为人工智能，机器人和“自主”系统的设计、生产、使用和治理规划一个整体化的、全面性的发展进程，为设立共同的、国际公认的道德框架铺平道路。


## 3. 对“自主”武器系统的思考
许多人很可能都看过一个“杀人蜂无人机”的发布视频。视频中伯克利大学教授Stuart Russell介绍了一种由AI控制的武装无人机。内含3g高能炸药的无人机体积只有柠檬大小，由AI控制飞行，可以通过自主人脸识别技术识别目标并进行攻击。
![](images\Slaughterbots.jpeg)
Image: Slaughterbots/YouTube  

事实上，“杀人蜂无人机”只是一个概念，在发布视频后的片尾，从事人工智能伯克利大学教授Stuart Russell最后教授也说明了，视频的主要内容为虚构，目的是警醒人类这种杀人机器人的崛起和AI技术的可怕用途。  
>AI 杀人蜂无人机发布视频：[链接1](https://open.163.com/movie/2017/11/0/9/MD32SIIIB_MD32SJS09.html) [链接2](http://autonomousweapons.org/slaughterbots/)

